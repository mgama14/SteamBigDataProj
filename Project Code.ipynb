{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcbca02-acf8-4dcf-96bd-03e660af648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appendix A\n",
    "\n",
    "\n",
    "kaggle datasets download -d kieranpoc/steam-reviews\n",
    "unzip steam-reviews.zip\n",
    "gcloud storage buckets create gs://gamasteamreviews --project=gamasteam \\ --default-storage-class=STANDARD --location=us-central1 --uniform-bucket-level-access\n",
    "gcloud storage cp all_reviews/all_reviews.csv gs://gamasteamreviews/landing/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726c9a4-372b-4760-b96d-2fe078500fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appendix B\n",
    "\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import functions as F\n",
    "csv = \"gs://gamasteamreviews/landing/all_reviews.csv\"\n",
    "df = spark.read.csv(csv, header=True, inferSchema=True, multiLine=True, escape='\"')\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\"gs://gamasteamreviews/landing/all_reviews.parquet\")\n",
    "df = spark.read.parquet(\"gs://gamasteamreviews/landing/all_reviews.parquet\")\n",
    "#I converted it to parquet to try and alleviate some performance issues\n",
    "#the record counts\n",
    "df.cache\n",
    "df.count()\n",
    "\n",
    "#the columns and data types\n",
    "\n",
    "df.printSchema()\n",
    "#handling the null values in the data\n",
    "\n",
    "null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "null_counts_pandas = null_counts.toPandas().transpose()\n",
    "null_counts_pandas.columns = [\"Null Count\"]\n",
    "null_counts_pandas.style\n",
    "stats = df.select(\n",
    "    \"author_num_games_owned\",\n",
    "    \"author_num_reviews\",\n",
    "    \"author_playtime_forever\",\n",
    "    \"author_playtime_last_two_weeks\",\n",
    "    \"author_playtime_at_review\",\n",
    "    \"author_last_played\",\n",
    "    \"voted_up\",\n",
    "    \"votes_up\",\n",
    "    \"votes_funny\",\n",
    "    \"weighted_vote_score\"\n",
    ").summary(\"count\", \"min\", \"max\", \"mean\",\"stddev\")\n",
    "\n",
    "stats_pandas = stats.toPandas()\n",
    "\n",
    "stats_pandas.style\n",
    "\n",
    "#the stats for the dates\n",
    "\n",
    "date_stats = df.select(\n",
    "    F.from_unixtime(\"timestamp_created\").alias(\"created_date\"),\n",
    "    F.from_unixtime(\"timestamp_updated\").alias(\"updated_date\")\n",
    "    ).select(\n",
    "    F.min(\"created_date\").alias(\"min_created_date\"),\n",
    "    F.max(\"created_date\").alias(\"max_created_date\"),\n",
    "    F.min(\"updated_date\").alias(\"min_updated_date\"),\n",
    "    F.max(\"updated_date\").alias(\"max_updated_date\")\n",
    ")\n",
    "\n",
    "date_summary_pandas = date_stats.toPandas()\n",
    "date_summary_pandas.style\n",
    "#review statistics\n",
    "\n",
    "df = df.withColumn(\"review_word_count\", F.size(F.split(F.col(\"review\"), \" \")))\n",
    "review_stats = df.agg(\n",
    "    F.min(\"review_word_count\").alias(\"min_word_count\"),\n",
    "    F.max(\"review_word_count\").alias(\"max_word_count\"),\n",
    "    F.avg(\"review_word_count\").alias(\"avg_word_count\")\n",
    ")\n",
    "review_stats_pandas = review_stats.toPandas()\n",
    "review_stats_pandas.style\n",
    "#review count by language\n",
    "review_count_by_language = df.groupBy(\"language\").count().orderBy(\"count\", ascending=False).limit(5).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=review_count_by_language, x=\"language\", y=\"count\", palette=\"viridis\")\n",
    "plt.title(\"Review Count by Language (Top 5 Languages)\")\n",
    "plt.xlabel(\"Language\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.show()\n",
    "\n",
    "#number of reviews for playtimes\n",
    "df_sample = df.sample(0.1).select(\n",
    "    (F.col(\"author_playtime_forever\") / 60).alias(\"author_playtime_hours\"), \n",
    "    \"author_num_reviews\"\n",
    ").toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_sample, x=\"author_playtime_hours\", y=\"author_num_reviews\", alpha=0.6)\n",
    "plt.title(\"Playtime (Hours) vs. Number of Reviews\")\n",
    "plt.xlabel(\"Playtime (Hours)\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.show()\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c26803-e108-425d-982e-2b115bdd271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appendix C\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType, BooleanType, LongType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "csv = \"gs://gamasteamreviews/landing/all_reviews.csv\"\n",
    "\n",
    "df = spark.read.csv(\n",
    "    csv,\n",
    "    header=True,        \n",
    "    inferSchema=True,   \n",
    "    multiLine=True,     \n",
    "    escape='\"'          \n",
    ")\n",
    "df.write.mode(\"overwrite\").parquet(\"gs://gamasteamreviews/landing/all_reviews_parquet\")\n",
    "parquet_path = \"gs://gamasteamreviews/landing/all_reviews_parquet\"\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"author_num_games_owned\",\n",
    "    \"author_num_reviews\",\n",
    "    \"author_playtime_forever\",\n",
    "    \"author_playtime_last_two_weeks\",\n",
    "    \"author_playtime_at_review\",\n",
    "    \"author_last_played\",\n",
    "    \"language\",\n",
    "    \"voted_up\",\n",
    "    \"steam_purchase\",\n",
    "    \"received_for_free\",\n",
    "    \"timestamp_created\",\n",
    "    \"timestamp_updated\"\n",
    "]\n",
    "\n",
    "df = df.select(*columns_to_keep)\n",
    "\n",
    "for col in [\"author_num_games_owned\", \"author_num_reviews\", \"author_playtime_forever\",\n",
    "            \"author_playtime_last_two_weeks\", \"author_playtime_at_review\", \"author_last_played\"]:\n",
    "    median_value = df.approxQuantile(col, [0.5], 0.05)[0]\n",
    "    df = df.fillna({col: median_value})\n",
    "\n",
    "df = df.fillna({\n",
    "    \"voted_up\": False,\n",
    "    \"steam_purchase\": False,\n",
    "    \"received_for_free\": False,\n",
    "})\n",
    "\n",
    "df = df.fillna({\"language\": \"unknown\"})\n",
    "cleaned_parquet_path = \"gs://gamasteamreviews/cleaned/all_reviews_cleaned.parquet\"\n",
    "df.write.mode(\"overwrite\").parquet(cleaned_parquet_path)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc2983-5e56-465c-966c-6ebd652daffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appendix D\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when, col, from_unixtime, hour\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "df = spark.read.parquet(\"gs://gamasteamreviews/cleaned/all_reviews_cleaned.parquet\")\n",
    "#with this dataset containing unix data for when the reviews are written, it's possible that the time of day affects review sentiment.\n",
    "#perhaps gamers are more likely to leave positive reviews in the afternoon, or night. \n",
    "df = df.withColumn(\n",
    "    \"time_of_day\",\n",
    "    when((hour(from_unixtime(col(\"timestamp_created\"))) >= 6) & (hour(from_unixtime(col(\"timestamp_created\"))) < 12), \"morning\")\n",
    "    .when((hour(from_unixtime(col(\"timestamp_created\"))) >= 12) & (hour(from_unixtime(col(\"timestamp_created\"))) < 18), \"afternoon\")\n",
    "    .when((hour(from_unixtime(col(\"timestamp_created\"))) >= 18) & (hour(from_unixtime(col(\"timestamp_created\"))) < 24), \"evening\")\n",
    "    .otherwise(\"night\"))\n",
    "\n",
    "    #this feature shows us the amount of time that has passed between when the reviewer had played the game, and when they reviewed it\n",
    "#the idea is the shorter the gap between the time last played and time of review, the stronger the reviewer will feel about the game due to recency bias\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"recency_bias\",\n",
    "    (col(\"timestamp_created\") - col(\"author_last_played\")) / (24 * 60 * 60)\n",
    ")\n",
    "\n",
    "#this is a feature piggybacking off the previous one, it checks to see if the author has kept playing the game after their review.\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"played_after_review\",\n",
    "    F.when(col(\"author_last_played\") > col(\"timestamp_created\"), 1).otherwise(0)\n",
    ")\n",
    "#filling any missing data in our newly created numeric columns\n",
    "\n",
    "for col_name in [\n",
    "    \"recency_bias\",\n",
    "    \"played_after_review\"\n",
    "]:\n",
    "    median_value = df.approxQuantile(col_name, [0.5], 0.05)[0]\n",
    "    df = df.fillna({col_name: median_value})\n",
    "#using the string indexer on our categorical values. using handleinvalid keep to deal with missing values\n",
    "\n",
    "language_indexer = StringIndexer(inputCol=\"language\", outputCol=\"language_indexed\", handleInvalid=\"keep\")\n",
    "time_of_day_indexer = StringIndexer(inputCol=\"time_of_day\", outputCol=\"time_of_day_indexed\", handleInvalid=\"keep\")\n",
    "#after using the string indexer and dealing with any missing values, we assemble our features into a vector\n",
    "\n",
    "feature_columns = [\n",
    "    \"author_num_games_owned\",\n",
    "    \"author_num_reviews\",\n",
    "    \"author_playtime_forever\",\n",
    "    \"author_playtime_last_two_weeks\",\n",
    "    \"author_playtime_at_review\",\n",
    "    \"author_last_played\",\n",
    "    \"recency_bias\",\n",
    "    \"played_after_review\",\n",
    "    \"language_indexed\",\n",
    "    \"time_of_day_indexed\"\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "#creating a pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[language_indexer, time_of_day_indexer, assembler])\n",
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "#creating our data isnto training and test sets\n",
    "\n",
    "train_data, test_data = df_transformed.randomSplit([0.7, 0.3], seed=42)\n",
    "#setting up for and executing k fold cross evaluation to find the parameters that will give us the best performance\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(labelCol=\"voted_up\", featuresCol=\"features\", maxDepth=15)\n",
    "\n",
    "params = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(dt_classifier.maxDepth, [5, 10, 15])\n",
    "    .addGrid(dt_classifier.maxBins, [32, 64])\n",
    "    .build()\n",
    ")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"voted_up\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "cross_val = CrossValidator(\n",
    "    estimator=dt_classifier,\n",
    "    estimatorParamMaps=params,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5 \n",
    ")\n",
    "\n",
    "cv_model = cross_val.fit(train_data)\n",
    "predictions = cv_model.bestModel.transform(test_data)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Cross-validated Accuracy: {accuracy}\")\n",
    "precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"voted_up\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"weightedPrecision\"\n",
    ").evaluate(predictions)\n",
    "print(f\"Precision: {precision}\")\n",
    "recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"voted_up\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"weightedRecall\"\n",
    ").evaluate(predictions)\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"voted_up\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\"\n",
    ").evaluate(predictions)\n",
    "print(f\"F1-Score: {f1}\")\n",
    "trusted_path = \"gs://gamasteamreviews/Trusted/all_reviews_with_features.parquet\"\n",
    "df_transformed.write.mode(\"overwrite\").parquet(trusted_path)\n",
    "models_path = \"gs://gamasteamreviews/Models/decision_tree_model\"\n",
    "cv_model.bestModel.write().overwrite().save(models_path)\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
